# ğŸ™ï¸ MIDAS

> **M**y **I**ntelligent **D**igital **A**ssistant **S**ystem â€” A fully offline, privacy-first voice assistant

[![License](https://img.shields.io/badge/license-MIT-gold.svg)](LICENSE)
[![Python](https://img.shields.io/badge/python-3.10+-blue.svg)](https://python.org)
[![CUDA](https://img.shields.io/badge/CUDA-11.8%2B-green.svg)](https://developer.nvidia.com/cuda-toolkit)

---

## âœ¨ What is MIDAS?

MIDAS is a **100% offline voice assistant** that runs entirely on your local machine. No cloud APIs, no data collection, no subscriptions â€” just you and your AI.

### Key Features

- ğŸ”’ **Completely Offline** â€” All processing happens locally
- ğŸ¤ **Voice Input** â€” Whisper-powered speech recognition
- ğŸ”Š **Voice Output** â€” Natural text-to-speech with Silero
- ğŸ§  **Conversational AI** â€” Powered by Hermes 3 LLM (3B parameters)
- ğŸ’¾ **Memory** â€” Remembers conversation context
- ğŸ“š **RAG Support** â€” Add your own knowledge documents
- âš¡ **GPU Accelerated** â€” Fast inference with CUDA

---

## ğŸ–¥ï¸ Requirements

| Component | Minimum | Recommended |
|-----------|---------|-------------|
| **OS** | Windows 10, Linux | Windows 11, Ubuntu 22.04 |
| **GPU** | NVIDIA 4GB VRAM | NVIDIA 6GB+ VRAM |
| **RAM** | 8 GB | 16 GB |
| **Python** | 3.10 | 3.11 |
| **CUDA** | 11.8 | 12.1+ |

> âš ï¸ **AMD/Intel GPUs**: Currently not supported. CPU-only mode is possible but slow.

---

## ğŸš€ Quick Start

### 1. Clone the Repository

```bash
git clone https://github.com/jadisreal/MIDAS.git
cd MIDAS
```

### 2. Create Virtual Environment

```bash
python -m venv .venv

# Windows (PowerShell)
.\.venv\Scripts\Activate.ps1

# Windows (CMD)
.venv\Scripts\activate.bat

# Linux/macOS
source .venv/bin/activate
```

### 3. Install PyTorch with CUDA

```bash
# CUDA 12.1 (RTX 30/40 series, recommended)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# CUDA 11.8 (older GPUs)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### 4. Install llama-cpp-python with CUDA

```powershell
# Windows (PowerShell)
$env:CMAKE_ARGS="-DGGML_CUDA=on"
pip install llama-cpp-python --force-reinstall --no-cache-dir
```

```bash
# Linux/macOS
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python --force-reinstall --no-cache-dir
```

### 5. Install Dependencies

```bash
pip install -r requirements.txt
```

### 6. Download AI Models

```bash
python download_models.py
```

This downloads:
- **Hermes 3 LLM** (~1.9 GB) â€” Language model
- **Silero TTS** (~55 MB) â€” Text-to-speech

### 7. Run MIDAS

```bash
python server.py
```

Open **http://localhost:8000** in your browser.

---

## ğŸ“ Project Structure

```
MIDAS/
â”œâ”€â”€ server.py              # FastAPI server entry point
â”œâ”€â”€ download_models.py     # Model downloader for first-time setup
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ index.html             # Web interface
â”œâ”€â”€ styles.css             # UI styling (Brutalist Gold theme)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py          # Configuration & settings
â”‚   â”œâ”€â”€ models.py          # Model loading (LLM, Whisper, TTS)
â”‚   â”œâ”€â”€ routes.py          # API endpoints
â”‚   â”œâ”€â”€ rag.py             # RAG knowledge base
â”‚   â””â”€â”€ utils.py           # Utility functions
â”‚
â”œâ”€â”€ models/                # LLM model files (downloaded)
â”‚   â””â”€â”€ *.gguf
â”‚
â”œâ”€â”€ knowledge/             # RAG documents (add your .txt files here)
â”‚   â””â”€â”€ *.txt
â”‚
â”œâ”€â”€ data/                  # User data
â”‚   â””â”€â”€ settings.json      # Persisted settings
â”‚
â””â”€â”€ model.pt               # Silero TTS model (downloaded)
```

---

## âš™ï¸ Configuration

Settings can be changed via the web UI or by editing `data/settings.json`:

| Setting | Default | Description |
|---------|---------|-------------|
| `contextWindow` | 4096 | LLM context size in tokens |
| `maxTokens` | 256 | Maximum response length |
| `temperature` | 0.7 | Creativity (0.0 = deterministic, 1.0 = creative) |
| `topP` | 0.9 | Nucleus sampling threshold |
| `voiceEnabled` | true | Enable/disable voice output |

---

## ğŸ“š Adding Knowledge (RAG)

Drop `.txt` files into the `knowledge/` folder. MIDAS will use them to answer questions.

Example use cases:
- Personal notes and documentation
- Product manuals
- Study materials
- Custom instructions

---

## ğŸ”§ Troubleshooting

### "CUDA not available" or slow inference
1. Verify NVIDIA drivers: `nvidia-smi`
2. Check CUDA: `nvcc --version`
3. Reinstall llama-cpp-python with CUDA flag (Step 4)

### Out of memory errors
- Close other GPU applications
- Use a smaller model quantization (Q3_K_S)
- Reduce `contextWindow` in settings

### Model loading fails
1. Re-run `python download_models.py`
2. Verify files exist in `models/` folder
3. Check file sizes match expected values

### Microphone not working
- Allow microphone access in browser
- Check browser console for errors
- Try a different browser (Chrome recommended)

---

## ğŸ¨ Theme

MIDAS features a **Brutalist Gold & Space** aesthetic â€” minimalist design with cosmic gold accents.

---

## ğŸ›£ï¸ Roadmap

- [ ] Multiple voice options
- [ ] Conversation export/import
- [ ] Plugin system
- [ ] Mobile-friendly UI
- [ ] Wake word detection

---

## ğŸ¤ Contributing

Contributions welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

MIT License â€” see [LICENSE](LICENSE) for details.

---

## ğŸ™ Acknowledgments

- [llama.cpp](https://github.com/ggerganov/llama.cpp) â€” LLM inference engine
- [faster-whisper](https://github.com/guillaumekln/faster-whisper) â€” Speech recognition
- [Silero Models](https://github.com/snakers4/silero-models) â€” Text-to-speech
- [Hermes 3](https://huggingface.co/NousResearch) â€” Language model by Nous Research
- [FastAPI](https://fastapi.tiangolo.com/) â€” Web framework

---

<p align="center">
  <b>Built with ğŸ”¥ for offline AI</b>
</p>

